{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960bba6f",
   "metadata": {
    "id": "960bba6f"
   },
   "source": [
    "# Building Predictive Models with Decision Trees and Random Forests\n",
    "\n",
    "\n",
    "Time estimate: **20** minutes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3abbe7",
   "metadata": {
    "id": "3e3abbe7"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Understand decision tree and random forest algorithms and when to use them.\n",
    "- Prepare tabular clinical data for tree-based models, including encoding and handling missing values.\n",
    "- Train, tune, and evaluate decision trees and random forests using scikit-learn.\n",
    "- Interpret model outputs: feature importance, partial dependence, and tree visualization.\n",
    "- Assess model performance with cross-validation, ROC/AUC, precision/recall, and calibration.\n",
    "- Communicate model limitations and suggestions for clinical deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14b32b",
   "metadata": {
    "id": "5b14b32b"
   },
   "source": [
    "## What you will do in this lab\n",
    "\n",
    "- Use a simulated clinical dataset for predicting a binary outcome (e.g., deterioration/readmission).\n",
    "- Train a decision tree and a random forest (RF) classifier.\n",
    "- Tune hyperparameters using grid search with cross-validation.\n",
    "- Inspect feature importances and visualize a sample tree.\n",
    "- Evaluate models on holdout set and compare metrics (ROC, AUC, precision/recall).\n",
    "- Use partial dependence plots for interpretation.\n",
    "- Complete 7 consolidated exercises with hints & solutions at the end of the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411019cd",
   "metadata": {
    "id": "411019cd"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Decision trees are intuitive, rule-based models that split features to predict outcomes. Random forests combine many trees to reduce variance and improve generalization. In clinical settings, tree-based models are popular for their interpretability and ability to handle mixed data types. This lab covers the end-to-end workflow: simulation, preprocessing, training, tuning, interpretation, and evaluation with practical advice for clinical use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3e189",
   "metadata": {
    "id": "02c3e189"
   },
   "source": [
    "## About the dataset/environment\n",
    "\n",
    "You will use a simulated dataset with patient demographics, vitals summaries, and simple lab flags to predict a binary clinical outcome 'event' (e.g., deterioration within 48 hours). The dataset includes numeric and categorical features, missing values, and class imbalance. Tools: Python (pandas, numpy, scikit-learn, matplotlib, seaborn, joblib).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89bf20",
   "metadata": {
    "id": "1c89bf20"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the following cell to install required libraries (if needed) and import all tools used for data preparation, model training, evaluation, and interpretation. If you are using Google Colab, run this cell to ensure all dependencies are available and your environment is set up consistently for building and analyzing tree-based machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713085e3",
   "metadata": {
    "id": "713085e3"
   },
   "outputs": [],
   "source": [
    "# Install required libraries quietly (useful in Colab environments)\n",
    "!pip -q install numpy pandas scikit-learn matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HoWk72e9NBMX",
   "metadata": {
    "id": "HoWk72e9NBMX"
   },
   "outputs": [],
   "source": [
    "# Import numerical computing library\n",
    "import numpy as np\n",
    "\n",
    "# Import data manipulation library\n",
    "import pandas as pd\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import model selection utilities\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "# Import tree-based models\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Import preprocessing utilities\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Import pipeline tools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Import model interpretation tools\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "\n",
    "# Import utility for saving models\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bBCtQOUCNI4f",
   "metadata": {
    "id": "bBCtQOUCNI4f"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae91b91",
   "metadata": {},
   "source": [
    "Run this code cell to load the clinical deterioration dataset directly from a remote GitHub repository and preview its structure by displaying the first few rows. This confirms that the data has been loaded correctly before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yFgM0JzILFwX",
   "metadata": {
    "id": "yFgM0JzILFwX"
   },
   "outputs": [],
   "source": [
    "# Load the dataset from a remote GitHub URL\n",
    "df = pd.read_csv('https://statistical-analysis-and-data-modeling-in-healthcare-02a43a.gitlab.io/labs/lab5/deterioration_data.csv')\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f526f9a",
   "metadata": {
    "id": "2f526f9a"
   },
   "source": [
    "## Step 1: Inspect and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc28eff",
   "metadata": {
    "id": "4bc28eff"
   },
   "source": [
    "Before modeling, inspect the dataset by reviewing its size, data types, summary statistics, outcome balance, and missing values. This helps surface preprocessing requirements and potential modeling risks early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd4f35",
   "metadata": {
    "id": "61bd4f35"
   },
   "outputs": [],
   "source": [
    "# Print number of rows and columns\n",
    "print('Rows, Columns:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8egV9366NhZI",
   "metadata": {
    "id": "8egV9366NhZI"
   },
   "outputs": [],
   "source": [
    "# Display data types of all columns\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bJVHmhz2NlPm",
   "metadata": {
    "id": "bJVHmhz2NlPm"
   },
   "outputs": [],
   "source": [
    "# Show descriptive statistics for numeric variables\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T5YnVEqiNqSh",
   "metadata": {
    "id": "T5YnVEqiNqSh"
   },
   "outputs": [],
   "source": [
    "# Display class counts for the outcome variable\n",
    "print('\\nEvent counts:\\n', df['event'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfcaab",
   "metadata": {
    "id": "e6cfcaab"
   },
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a4da5",
   "metadata": {
    "id": "035a4da5"
   },
   "source": [
    "## Step 2: Define preprocessing and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352c283",
   "metadata": {
    "id": "a352c283"
   },
   "source": [
    "Tree-based models can work with mixed data types but do not accept missing values. To prepare the data, you'll impute missing numeric values using the median, one-hot encode categorical variables, and combine all preprocessing steps into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a9819",
   "metadata": {
    "id": "0f1a9819"
   },
   "outputs": [],
   "source": [
    "# Define numeric feature columns\n",
    "numeric_features = ['age','hr_mean','sbp_mean','comorbidity_count','prior_adm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WHFy5xitN_V5",
   "metadata": {
    "id": "WHFy5xitN_V5"
   },
   "outputs": [],
   "source": [
    "# Create numeric preprocessing pipeline with median imputation\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HtQxZdk9N9Ch",
   "metadata": {
    "id": "HtQxZdk9N9Ch"
   },
   "outputs": [],
   "source": [
    "# Define categorical feature columns\n",
    "categorical_features = ['sex','lab_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WClM9Z6UODf8",
   "metadata": {
    "id": "WClM9Z6UODf8"
   },
   "outputs": [],
   "source": [
    "# Create categorical preprocessing pipeline with one-hot encoding\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('onehot', OneHotEncoder())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2CPTsrIAOIzp",
   "metadata": {
    "id": "2CPTsrIAOIzp"
   },
   "outputs": [],
   "source": [
    "# Combine numeric and categorical preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veyzZB48OLe_",
   "metadata": {
    "id": "veyzZB48OLe_"
   },
   "outputs": [],
   "source": [
    "# Separate predictors and outcome\n",
    "X = df.drop(columns=['patient_id','event'])\n",
    "y = df['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VmA8IWb2ORTk",
   "metadata": {
    "id": "VmA8IWb2ORTk"
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ogBOKacBN7lF",
   "metadata": {
    "id": "ogBOKacBN7lF"
   },
   "outputs": [],
   "source": [
    "# Print training data shape\n",
    "print('Training shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32089da5",
   "metadata": {
    "id": "32089da5"
   },
   "source": [
    "## Step 3: Train decision tree baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf17bd",
   "metadata": {
    "id": "87cf17bd"
   },
   "source": [
    "Now, let's train a baseline decision tree to establish a simple reference model, visualize the learned decision rules, and measure initial model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719e13d",
   "metadata": {
    "id": "b719e13d"
   },
   "outputs": [],
   "source": [
    "# Create a pipeline with preprocessing and decision tree classifier\n",
    "dt_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf', DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qWALzV49OhCs",
   "metadata": {
    "id": "qWALzV49OhCs"
   },
   "outputs": [],
   "source": [
    "# Train the decision tree model\n",
    "dt_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4k_OLuMxOlin",
   "metadata": {
    "id": "4k_OLuMxOlin"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities for the positive class\n",
    "y_prob_dt = dt_pipeline.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hVEB3TFuOqe2",
   "metadata": {
    "id": "hVEB3TFuOqe2"
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC score\n",
    "print('Decision Tree AUC:', round(roc_auc_score(y_test, y_prob_dt),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_o7iAVSqO8ol",
   "metadata": {
    "id": "_o7iAVSqO8ol"
   },
   "source": [
    "## Visualize the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d5a74",
   "metadata": {},
   "source": [
    "Run this code cell to visualize the structure of the trained decision tree model. The plot displays a shallow version of the tree (limited depth) with feature names, split rules, and class outcomes, helping you understand how the model makes clinical risk predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nPnUHB6PO1zE",
   "metadata": {
    "id": "nPnUHB6PO1zE"
   },
   "outputs": [],
   "source": [
    "# Create a large figure for readability\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Extract numeric feature names\n",
    "num_feats = list(preprocessor.transformers_[0][2])\n",
    "\n",
    "# Extract one-hot encoded feature names\n",
    "ohe_feats = list(\n",
    "    dt_pipeline.named_steps['preprocessor']\n",
    "    .named_transformers_['cat']\n",
    "    .named_steps['onehot']\n",
    "    .get_feature_names_out(['sex','lab_flag'])\n",
    ")\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = num_feats + ohe_feats\n",
    "\n",
    "# Plot a shallow version of the decision tree\n",
    "plot_tree(\n",
    "    dt_pipeline.named_steps['clf'],\n",
    "    feature_names=feature_names,\n",
    "    max_depth=3,\n",
    "    filled=True,\n",
    "    fontsize=8\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116611ee",
   "metadata": {
    "id": "116611ee"
   },
   "source": [
    "## Step 4: Train random forest and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a7be3",
   "metadata": {
    "id": "f45a7be3"
   },
   "source": [
    "Random forests reduce overfitting by averaging predictions from many decision trees. Let's train a baseline forest and examine feature importance to understand which variables drive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12Oz6x5qPXxB",
   "metadata": {
    "id": "12Oz6x5qPXxB"
   },
   "outputs": [],
   "source": [
    "# Create a pipeline with preprocessing and random forest classifier\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf', RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VJf_omJjPbsb",
   "metadata": {
    "id": "VJf_omJjPbsb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the random forest model\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_prob_rf = rf_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print('Random Forest AUC:', round(roc_auc_score(y_test, y_prob_rf),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dfa25",
   "metadata": {
    "id": "281dfa25"
   },
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "# Get feature names\n",
    "num_feats = list(preprocessor.transformers_[0][2])\n",
    "ohe = rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "cat_feats = list(ohe.get_feature_names_out(['sex','lab_flag']))\n",
    "feature_names = num_feats + cat_feats\n",
    "importances = rf_pipeline.named_steps['clf'].feature_importances_\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "feat_imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bad1ac",
   "metadata": {
    "id": "68bad1ac"
   },
   "source": [
    "## Step 5: Hyperparameter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505482b1",
   "metadata": {
    "id": "505482b1"
   },
   "source": [
    "Run this code cell to tune key random forest hyperparameters—such as tree depth, number of estimators, and maximum features—using cross-validation to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df93766",
   "metadata": {
    "id": "9df93766"
   },
   "outputs": [],
   "source": [
    "# Define grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [None, 6, 12],\n",
    "    'clf__max_features': ['sqrt', 0.5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xL0g3SrBP-9A",
   "metadata": {
    "id": "xL0g3SrBP-9A"
   },
   "outputs": [],
   "source": [
    "# Create GridSearchCV object\n",
    "grid = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SirN4XpTQBxD",
   "metadata": {
    "id": "SirN4XpTQBxD"
   },
   "outputs": [],
   "source": [
    "# Fit grid search on training data\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jp-z01kYQOU_",
   "metadata": {
    "id": "jp-z01kYQOU_"
   },
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print('Best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SfvpmG9UQR2u",
   "metadata": {
    "id": "SfvpmG9UQR2u"
   },
   "outputs": [],
   "source": [
    "# Extract best model\n",
    "best_rf = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lPZeU6CLQUHv",
   "metadata": {
    "id": "lPZeU6CLQUHv"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities with tuned model\n",
    "y_prob_best = best_rf.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cp75jHQWud",
   "metadata": {
    "id": "27cp75jHQWud"
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC\n",
    "print('Best RF AUC:', round(roc_auc_score(y_test, y_prob_best),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca8f44",
   "metadata": {
    "id": "20ca8f44"
   },
   "source": [
    "## Step 6: Interpretation - Feature importance and partial dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a53792",
   "metadata": {
    "id": "c5a53792"
   },
   "source": [
    "Run this code cell to explore how the most important input features influence model predictions using a partial dependence plot (PDP). This visualization shows the average effect of a selected feature on the predicted outcome while holding other features constant, helping you interpret the model’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NzUW5tHxQgR5",
   "metadata": {
    "id": "NzUW5tHxQgR5"
   },
   "outputs": [],
   "source": [
    "# Select top 3 most important features\n",
    "top_feats = feat_imp['feature'].head(3).tolist()\n",
    "\n",
    "# Create a figure for PDP\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# Generate partial dependence plot (simplified demo)\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    best_rf.named_steps['clf'],\n",
    "    best_rf.named_steps['preprocessor'].transform(X_train),\n",
    "    features=[0],\n",
    "    feature_names=feature_names,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Note: This is a simplified PDP demonstration for teaching purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7d16f",
   "metadata": {
    "id": "b3c7d16f"
   },
   "source": [
    "## Step 7: Evaluation, calibration, and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09092a8d",
   "metadata": {
    "id": "09092a8d"
   },
   "source": [
    "Run this code cell to compare the predictive performance of the baseline and tuned random forest models using AUC. You will then evaluate how well the tuned model’s predicted probabilities align with observed outcomes by generating a calibration curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-htiHeIPQyVF",
   "metadata": {
    "id": "-htiHeIPQyVF"
   },
   "outputs": [],
   "source": [
    "# Compare baseline and tuned Random Forest performance\n",
    "print('Baseline RF AUC:', round(roc_auc_score(y_test, y_prob_rf),3))\n",
    "print('Tuned RF AUC:', round(roc_auc_score(y_test, y_prob_best),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xyht61C0Q1p4",
   "metadata": {
    "id": "Xyht61C0Q1p4"
   },
   "outputs": [],
   "source": [
    "# Import calibration function\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Compute calibration curve values\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test, y_prob_best, n_bins=10\n",
    ")\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.title('Calibration plot (tuned RF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf5c65",
   "metadata": {
    "id": "ecaf5c65"
   },
   "source": [
    "## Consolidated practice exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf34f01",
   "metadata": {
    "id": "aaf34f01"
   },
   "source": [
    "### Exercise 1: Inspect data & missingness, report event rate and null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524890fb",
   "metadata": {
    "id": "524890fb"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6323b68",
   "metadata": {
    "id": "e6323b68"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Use df.isna().sum() and df['event'].value_counts(normalize=True).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff95a30",
   "metadata": {
    "id": "4ff95a30"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "print(df.isna().sum())\n",
    "print('Event rate:', df['event'].mean())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d3aa9",
   "metadata": {
    "id": "427d3aa9"
   },
   "source": [
    "### Exercise 2: Build preprocessing pipeline and show transformed training shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6cff7",
   "metadata": {
    "id": "dfa6cff7"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f98e6",
   "metadata": {
    "id": "f98f98e6"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Use preprocessor.fit_transform(X_train) or pipeline to inspect shape.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad42163",
   "metadata": {
    "id": "dad42163"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "Xt = preprocessor.fit_transform(X_train)\n",
    "print('Transformed shape:', Xt.shape)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8187db0",
   "metadata": {
    "id": "d8187db0"
   },
   "source": [
    "### Exercise 3: Train a decision tree with max_depth=6 and report AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98daa61",
   "metadata": {
    "id": "d98daa61"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acae92d",
   "metadata": {
    "id": "3acae92d"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Set DecisionTreeClassifier(max_depth=6) in pipeline and compute roc_auc_score.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa0fc6",
   "metadata": {
    "id": "86fa0fc6"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "dt = Pipeline(steps=[('preprocessor', preprocessor), ('clf', DecisionTreeClassifier(max_depth=6, random_state=42))])\n",
    "dt.fit(X_train, y_train)\n",
    "y_prob = dt.predict_proba(X_test)[:,1]\n",
    "print('AUC:', roc_auc_score(y_test, y_prob))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721931d",
   "metadata": {
    "id": "8721931d"
   },
   "source": [
    "### Exercise 4: Train random forest with 200 trees and report feature importances (top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0d59b",
   "metadata": {
    "id": "abb0d59b"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b64b78",
   "metadata": {
    "id": "47b64b78"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Use RandomForestClassifier(n_estimators=200).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc66f4",
   "metadata": {
    "id": "d5fc66f4"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor), ('clf', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "rf.fit(X_train, y_train)\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': rf.named_steps['clf'].feature_importances_}).sort_values('importance', ascending=False)\n",
    "feat_imp.head(5)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78440a4b",
   "metadata": {
    "id": "78440a4b"
   },
   "source": [
    "### Exercise 5: Perform GridSearchCV (small grid) and report best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda89a51",
   "metadata": {
    "id": "cda89a51"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f11c5",
   "metadata": {
    "id": "187f11c5"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Use the param_grid shown above and GridSearchCV(cv=3).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6de3ad",
   "metadata": {
    "id": "1e6de3ad"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "grid = GridSearchCV(rf_pipeline, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b1342",
   "metadata": {
    "id": "ce7b1342"
   },
   "source": [
    "### Exercise 6: Plot partial dependence for the top feature and interpret shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040f0f0",
   "metadata": {
    "id": "1040f0f0"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff0db1",
   "metadata": {
    "id": "acff0db1"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Use PartialDependenceDisplay.from_estimator on best_rf.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc613a",
   "metadata": {
    "id": "a1bc613a"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "top = feat_imp['feature'].iloc[0]\n",
    "PartialDependenceDisplay.from_estimator(best_rf.named_steps['clf'], best_rf.named_steps['preprocessor'].transform(X_train), features=[0], feature_names=feature_names)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc616e",
   "metadata": {
    "id": "3bdc616e"
   },
   "source": [
    "### Exercise 7: Compare tuned RF and baseline RF AUC on test set and choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23c7e1",
   "metadata": {
    "id": "9f23c7e1"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcacd4",
   "metadata": {
    "id": "7efcacd4"
   },
   "source": [
    "<details> <summary>Click here for a hint</summary>\n",
    "\n",
    "Compute roc_auc_score for y_prob_rf and y_prob_best and pick higher AUC.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a51b3c",
   "metadata": {
    "id": "e6a51b3c"
   },
   "source": [
    "<details> <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "print('Baseline RF AUC:', roc_auc_score(y_test, y_prob_rf))\n",
    "print('Tuned RF AUC:', roc_auc_score(y_test, y_prob_best))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83ed6d",
   "metadata": {
    "id": "ee83ed6d"
   },
   "source": [
    "## Final thoughts and best practices\n",
    "\n",
    "- Tree-based models handle mixed data types and missing values well but can overfit—use tuning and ensembling.  \n",
    "- Random forests reduce variance but lose some interpretability; partial dependence plots and feature importances help.  \n",
    "- Validate models with cross-validation and assess calibration before deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697ca21",
   "metadata": {
    "id": "1697ca21"
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully completed this lab on **Building Predictive Models with Decision Trees and Random Forests**.\n",
    "\n",
    "In this lab, you built and compared tree-based machine learning models to predict a binary clinical outcome using a simulated dataset. You trained a baseline decision tree and a random forest, examined feature importance, and visualized decision rules to understand how predictions are made.\n",
    "\n",
    "You then tuned random forest hyperparameters using cross-validation, evaluated models with ROC/AUC and calibration plots, and used partial dependence plots to interpret key predictors.\n",
    "\n",
    "By the end of the lab, you practiced selecting, tuning, and interpreting tree-based models, and summarizing their strengths, limitations, and readiness for clinical deployment.\n",
    "\n",
    "## Authors\n",
    "\n",
    "Ramesh Sannareddy\n",
    "\n",
    "Copyright © 2025 SkillUp. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
